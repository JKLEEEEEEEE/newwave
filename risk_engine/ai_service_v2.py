"""
AI ì„œë¹„ìŠ¤ v2 - OpenAI ì‹¤ì œ ì—°ë™
Risk Monitoring System v2.3

7ëŒ€ AI ê¸°ëŠ¥:
1. analyze_news - ë‰´ìŠ¤ ìë™ ë¶„ì„
2. summarize_risk - ë¦¬ìŠ¤í¬ ìš”ì•½
3. text_to_cypher - ìì—°ì–´ â†’ Cypher ë³€í™˜
4. interpret_simulation - ì‹œë®¬ë ˆì´ì…˜ í•´ì„
5. explain_propagation - ì „ì´ ê²½ë¡œ ì„¤ëª…
6. generate_action_guide - RM/OPS ëŒ€ì‘ ê°€ì´ë“œ
7. generate_comprehensive_insight - ì¢…í•© ì¸ì‚¬ì´íŠ¸ ë¶„ì„ (NEW)
   - ë¦¬ìŠ¤í¬ ì ìˆ˜ëŠ” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê³„ì‚°ë¨, AIëŠ” ë§¥ë½ì  í•´ì„ ì œê³µ
   - ì—…ê³„ ì»¨í…ìŠ¤íŠ¸, ì‹ í˜¸ êµì°¨ ë¶„ì„, íŒ¨í„´ ì¸ì‹, ê¶Œê³ ì‚¬í•­ ìƒì„±
"""

import os
import json
import hashlib
import logging
from typing import Dict, Any, Optional
from functools import lru_cache
from datetime import datetime, timedelta

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš ï¸ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜. pip install openai")

from dotenv import load_dotenv
load_dotenv('.env.local')

logger = logging.getLogger(__name__)


class AIServiceV2:
    """6ëŒ€ AI ê¸°ëŠ¥ êµ¬í˜„ (OpenAI ì—°ë™)"""

    def __init__(self):
        self._client = None
        self.model = os.getenv("OPENAI_MODEL", "gpt-5.2")
        self.max_tokens = int(os.getenv("OPENAI_MAX_TOKENS", 2000))
        self.temperature = float(os.getenv("OPENAI_TEMPERATURE", 0.7))

        # ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ìºì‹œ (TTL 1ì‹œê°„)
        self._cache: Dict[str, Dict[str, Any]] = {}
        self._cache_ttl = int(os.getenv("OPENAI_CACHE_TTL_SECONDS", 3600))

    @property
    def client(self) -> Optional[OpenAI]:
        """OpenAI í´ë¼ì´ì–¸íŠ¸ (ì§€ì—° ì´ˆê¸°í™”)"""
        if self._client is None and OPENAI_AVAILABLE:
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                self._client = OpenAI(api_key=api_key)
        return self._client

    @property
    def is_available(self) -> bool:
        """AI ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return OPENAI_AVAILABLE and self.client is not None

    @property
    def enrichment_enabled(self) -> bool:
        """AI enrichment í™œì„±í™” ì—¬ë¶€ (í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´)"""
        return os.getenv("AI_ENRICHMENT_ENABLED", "false").lower() == "true"

    def _get_cache_key(self, func_name: str, params: dict) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        param_str = json.dumps(params, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(f"{func_name}:{param_str}".encode()).hexdigest()

    def _get_cached(self, cache_key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ì¡°íšŒ"""
        if cache_key in self._cache:
            entry = self._cache[cache_key]
            if datetime.now() < entry["expires"]:
                return entry["data"]
            else:
                del self._cache[cache_key]
        return None

    def _set_cache(self, cache_key: str, data: Any):
        """ìºì‹œì— ì €ì¥"""
        self._cache[cache_key] = {
            "data": data,
            "expires": datetime.now() + timedelta(seconds=self._cache_ttl)
        }

    def _call_gpt(self, system_prompt: str, user_prompt: str, use_json: bool = True) -> str:
        """GPT API í˜¸ì¶œ"""
        if not self.is_available:
            raise RuntimeError("OpenAI ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€")

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        kwargs = {
            "model": self.model,
            "messages": messages,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature
        }

        if use_json:
            kwargs["response_format"] = {"type": "json_object"}

        response = self.client.chat.completions.create(**kwargs)
        return response.choices[0].message.content

    # ========================================
    # 1. ë‰´ìŠ¤ ìë™ ë¶„ì„
    # ========================================
    def analyze_news(self, news_content: str, title: str = "") -> Dict[str, Any]:
        """ë‰´ìŠ¤ ë³¸ë¬¸ â†’ ë¦¬ìŠ¤í¬ ë¶„ì„"""

        # ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key("analyze_news", {"content": news_content[:200]})
        cached = self._get_cached(cache_key)
        if cached:
            return cached

        if not self.is_available:
            return self._fallback_analyze_news(news_content, title)

        system_prompt = """ë‹¹ì‹ ì€ ê¸ˆìœµ ë¦¬ìŠ¤í¬ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì—¬ ì •í™•íˆ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{
  "severity": 1-5 (1=ë‚®ìŒ, 5=ë§¤ìš°ë†’ìŒ),
  "category": "financial|legal|governance|supply_chain|market|reputation|operational|macro",
  "affected_companies": ["íšŒì‚¬ëª…1", "íšŒì‚¬ëª…2"],
  "summary": "í•œ ì¤„ ìš”ì•½ (30ì ì´ë‚´)",
  "risk_factors": ["ë¦¬ìŠ¤í¬1", "ë¦¬ìŠ¤í¬2"],
  "confidence": 0.0-1.0
}"""

        user_prompt = f"ì œëª©: {title}\n\në³¸ë¬¸:\n{news_content[:1500]}"

        try:
            result = self._call_gpt(system_prompt, user_prompt)
            parsed = json.loads(result)
            self._set_cache(cache_key, parsed)
            return parsed
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._fallback_analyze_news(news_content, title)

    def _fallback_analyze_news(self, content: str, title: str) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ë¶„ì„ í´ë°±"""
        text = f"{title} {content}".lower()

        # í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ ë¶„ì„
        severity = 3
        category = "operational"

        if any(kw in text for kw in ["ì†Œì†¡", "ê²€ì°°", "ê¸°ì†Œ", "ì¡°ì‚¬"]):
            severity = 4
            category = "legal"
        elif any(kw in text for kw in ["ì†ì‹¤", "ì ì", "í•˜ë½"]):
            severity = 3
            category = "financial"
        elif any(kw in text for kw in ["ê³µê¸‰", "ë¶€í’ˆ", "ë‚©í’ˆ"]):
            category = "supply_chain"

        return {
            "severity": severity,
            "category": category,
            "affected_companies": [],
            "summary": title[:30] if title else "ë¶„ì„ ë¶ˆê°€",
            "risk_factors": ["ìë™ ë¶„ì„ í•„ìš”"],
            "confidence": 0.3
        }

    # ========================================
    # 2. ë¦¬ìŠ¤í¬ ìš”ì•½
    # ========================================
    def summarize_risk(self, deal_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë”œ ë°ì´í„° â†’ ë¦¬ìŠ¤í¬ ìš”ì•½"""

        if not self.is_available:
            return self._fallback_summarize_risk(deal_data)

        system_prompt = """ë‹¹ì‹ ì€ PE/IB ì‹¬ì‚¬ì—­ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë”œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì •í™•íˆ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{
  "one_liner": "í•œ ë¬¸ì¥ í•µì‹¬ ìš”ì•½ (50ì ì´ë‚´)",
  "key_risks": ["í•µì‹¬ ë¦¬ìŠ¤í¬1", "í•µì‹¬ ë¦¬ìŠ¤í¬2", "í•µì‹¬ ë¦¬ìŠ¤í¬3"],
  "recommendation": "ê¶Œì¥ í–‰ë™ (í•œ ë¬¸ì¥)",
  "confidence": 0.0-1.0
}"""

        try:
            result = self._call_gpt(system_prompt, json.dumps(deal_data, ensure_ascii=False))
            return json.loads(result)
        except Exception as e:
            logger.error(f"ë¦¬ìŠ¤í¬ ìš”ì•½ ì‹¤íŒ¨: {e}")
            return self._fallback_summarize_risk(deal_data)

    def _fallback_summarize_risk(self, deal_data: Dict) -> Dict[str, Any]:
        """ë¦¬ìŠ¤í¬ ìš”ì•½ í´ë°±"""
        name = deal_data.get("name", "ëŒ€ìƒ ê¸°ì—…")
        score = deal_data.get("score", 50)

        status = "ì •ìƒ" if score <= 40 else "ì£¼ì˜" if score <= 70 else "ìœ„í—˜"

        return {
            "one_liner": f"{name}ì€ í˜„ì¬ {status} ìƒíƒœì…ë‹ˆë‹¤. ì§€ì† ëª¨ë‹ˆí„°ë§ í•„ìš”.",
            "key_risks": ["ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ìƒì„¸ ë¶„ì„ ë¶ˆê°€"],
            "recommendation": "ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ í›„ ì¬ë¶„ì„ í•„ìš”",
            "confidence": 0.3
        }

    # ========================================
    # 3. Text2Cypher
    # ========================================
    def text_to_cypher(self, natural_query: str) -> Dict[str, Any]:
        """ìì—°ì–´ â†’ Cypher ì¿¼ë¦¬ ë³€í™˜ (v5 5-Node ìŠ¤í‚¤ë§ˆ)"""

        if not self.is_available:
            return self._fallback_text_to_cypher(natural_query)

        system_prompt = """ë‹¹ì‹ ì€ Neo4j Cypher ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ìì—°ì–´ ì§ˆë¬¸ì„ Cypher ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ì„¸ìš”.

=== ê·¸ë˜í”„ ìŠ¤í‚¤ë§ˆ (5-Node Hierarchy) ===

ë…¸ë“œ:
- (:Deal {name, targetCompanyName, analyst, status, stage})
- (:Company {name, sector, market, totalRiskScore, directScore, propagatedScore, riskLevel})
- (:RiskCategory {code, name, riskScore, weight, weightedScore, eventCount})
  - code: SHARE, EXEC, CREDIT, LEGAL, GOV, OPS, AUDIT, ESG, SUPPLY, OTHER
- (:RiskEntity {name, type, position, description, riskScore})
  - type: PERSON, SHAREHOLDER, CASE, ISSUE
- (:RiskEvent {title, type, severity, score, source, publishedAt, summary})
  - type: NEWS, DISCLOSURE, ISSUE

ê´€ê³„:
- (Deal)-[:TARGET]->(Company)         -- ë”œì˜ ë©”ì¸ ê¸°ì—…
- (Company)-[:HAS_CATEGORY]->(RiskCategory) -- ê¸°ì—…ì˜ 10ê°œ ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬
- (Company)-[:HAS_RELATED]->(Company)       -- ê´€ë ¨ê¸°ì—…
- (RiskCategory)-[:HAS_ENTITY]->(RiskEntity) -- ì¹´í…Œê³ ë¦¬ ë‚´ ì—”í‹°í‹°
- (RiskEntity)-[:HAS_EVENT]->(RiskEvent)     -- ì—”í‹°í‹°ì˜ ì´ë²¤íŠ¸/ë‰´ìŠ¤

ê·œì¹™:
1. ì½ê¸° ì „ìš© ì¿¼ë¦¬ë§Œ (MATCH, RETURN, WHERE, ORDER BY, WITH, OPTIONAL MATCH)
2. DELETE, CREATE, SET, MERGE, REMOVE, DROP ì ˆëŒ€ ê¸ˆì§€
3. LIMIT 20 ê¸°ë³¸ ì ìš©
4. í•œêµ­ì–´ íšŒì‚¬ëª… ì‚¬ìš© (ì˜ˆ: 'SKí•˜ì´ë‹‰ìŠ¤', 'ì‚¼ì„±ì „ì')

ì •í™•íˆ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{
  "cypher": "MATCH ... RETURN ...",
  "explanation": "ì¿¼ë¦¬ ì„¤ëª… (í•œêµ­ì–´)"
}"""

        try:
            result = self._call_gpt(system_prompt, natural_query)
            parsed = json.loads(result)

            # ë³´ì•ˆ: ìœ„í—˜í•œ í‚¤ì›Œë“œ ê²€ì‚¬
            cypher = parsed.get("cypher", "")
            self._validate_cypher_safety(cypher)

            return parsed
        except ValueError as e:
            return {"cypher": None, "explanation": str(e), "error": True}
        except Exception as e:
            logger.error(f"Text2Cypher ì‹¤íŒ¨: {e}")
            return self._fallback_text_to_cypher(natural_query)

    def generate_answer(self, question: str, cypher: str, results: list) -> str:
        """Cypher ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ìì—°ì–´ ë‹µë³€ìœ¼ë¡œ ë³€í™˜"""
        if not self.is_available:
            return self._fallback_answer(question, results)

        results_str = json.dumps(results[:10], ensure_ascii=False, default=str) if results else "ê²°ê³¼ ì—†ìŒ"

        system_prompt = """ë‹¹ì‹ ì€ íˆ¬ì ë¦¬ìŠ¤í¬ ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
Neo4j ê·¸ë˜í”„ DB ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ í•œêµ­ì–´ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

ê·œì¹™:
1. ê²°ê³¼ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œ
2. ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ (3-5ë¬¸ì¥)
3. ìˆ«ì/ì ìˆ˜ê°€ ìˆìœ¼ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©
4. ê²°ê³¼ê°€ ì—†ìœ¼ë©´ "í•´ë‹¹ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ì•ˆë‚´
5. ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜ (JSON ì•„ë‹˜)"""

        user_msg = f"ì§ˆë¬¸: {question}\nCypher: {cypher}\nê²°ê³¼: {results_str}"

        try:
            return self._call_gpt(system_prompt, user_msg)
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._fallback_answer(question, results)

    def _fallback_answer(self, question: str, results: list) -> str:
        """AI ì—†ì„ ë•Œ ê²°ê³¼ ê¸°ë°˜ ê°„ë‹¨ ë‹µë³€"""
        if not results:
            return "í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”."
        count = len(results)
        return f"ì´ {count}ê±´ì˜ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ìƒì„¸ ë‚´ìš©ì€ ì•„ë˜ ê²°ê³¼ í…Œì´ë¸”ì„ í™•ì¸í•˜ì„¸ìš”."

    def _validate_cypher_safety(self, cypher: str):
        """Cypher ì¿¼ë¦¬ ì•ˆì „ì„± ê²€ì¦"""
        dangerous_keywords = ["DELETE", "CREATE", "SET ", "MERGE", "REMOVE", "DROP", "DETACH"]
        cypher_upper = cypher.upper()

        for keyword in dangerous_keywords:
            if keyword in cypher_upper:
                raise ValueError(f"ìœ„í—˜í•œ ì¿¼ë¦¬ ê°ì§€: {keyword} í‚¤ì›Œë“œ ì‚¬ìš© ë¶ˆê°€")

    def _fallback_text_to_cypher(self, query: str) -> Dict[str, Any]:
        """Text2Cypher í´ë°± (v5 5-Node ìŠ¤í‚¤ë§ˆ)"""
        query_lower = query.lower()

        if "ë²•ë¥ " in query_lower or "legal" in query_lower:
            return {
                "cypher": "MATCH (c:Company)-[:HAS_CATEGORY]->(rc:RiskCategory {code: 'LEGAL'}) RETURN c.name, rc.riskScore, rc.weightedScore, rc.eventCount ORDER BY rc.riskScore DESC LIMIT 20",
                "explanation": "ë²•ë¥ (LEGAL) ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤í¬ ì ìˆ˜ ì¡°íšŒ"
            }
        elif "ê³µê¸‰" in query_lower or "supply" in query_lower:
            return {
                "cypher": "MATCH (c:Company)-[:HAS_RELATED]->(rc:Company) RETURN c.name AS main, rc.name AS related, rc.totalRiskScore ORDER BY rc.totalRiskScore DESC LIMIT 20",
                "explanation": "ê´€ë ¨ê¸°ì—…(ê³µê¸‰ë§) ë¦¬ìŠ¤í¬ ì „ì´ ê´€ê³„ ì¡°íšŒ"
            }
        elif "ê³ ìœ„í—˜" in query_lower or "ìœ„í—˜" in query_lower or "ë¦¬ìŠ¤í¬" in query_lower:
            return {
                "cypher": "MATCH (c:Company) RETURN c.name, c.totalRiskScore, c.riskLevel, c.directScore, c.propagatedScore ORDER BY c.totalRiskScore DESC LIMIT 20",
                "explanation": "ê¸°ì—…ë³„ ë¦¬ìŠ¤í¬ ì ìˆ˜ ì¡°íšŒ"
            }
        elif "ì„ì›" in query_lower or "ê²½ì˜" in query_lower or "exec" in query_lower:
            return {
                "cypher": "MATCH (c:Company)-[:HAS_CATEGORY]->(rc:RiskCategory {code: 'EXEC'})-[:HAS_ENTITY]->(re:RiskEntity) RETURN c.name, re.name, re.type, re.position, re.riskScore ORDER BY re.riskScore DESC LIMIT 20",
                "explanation": "ê²½ì˜ì§„(EXEC) ê´€ë ¨ ì—”í‹°í‹° ì¡°íšŒ"
            }
        elif "ì£¼ì£¼" in query_lower or "share" in query_lower:
            return {
                "cypher": "MATCH (c:Company)-[:HAS_CATEGORY]->(rc:RiskCategory {code: 'SHARE'})-[:HAS_ENTITY]->(re:RiskEntity) RETURN c.name, re.name, re.type, re.riskScore ORDER BY re.riskScore DESC LIMIT 20",
                "explanation": "ì£¼ì£¼(SHARE) ê´€ë ¨ ì—”í‹°í‹° ì¡°íšŒ"
            }
        else:
            return {
                "cypher": "MATCH (d:Deal)-[:TARGET]->(c:Company) OPTIONAL MATCH (c)-[:HAS_CATEGORY]->(rc:RiskCategory) RETURN d.name AS deal, c.name AS company, c.totalRiskScore, collect(rc.code + ': ' + toString(rc.riskScore))[..5] AS topCategories LIMIT 20",
                "explanation": "ë”œ ë° ë©”ì¸ê¸°ì—… ë¦¬ìŠ¤í¬ ìš”ì•½ ì¡°íšŒ"
            }

    # ========================================
    # 4. ì‹œë®¬ë ˆì´ì…˜ í•´ì„
    # ========================================
    def interpret_simulation(self, simulation_result: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ â†’ ë¹„ì¦ˆë‹ˆìŠ¤ í•´ì„"""

        if not self.is_available:
            return self._fallback_interpret_simulation(simulation_result)

        system_prompt = """ë‹¹ì‹ ì€ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ì—ì„œ í•´ì„í•˜ì„¸ìš”.

ì •í™•íˆ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{
  "impact_summary": "ì˜í–¥ ìš”ì•½ (50ì ì´ë‚´)",
  "most_affected": "ê°€ì¥ í° ì˜í–¥ì„ ë°›ëŠ” ì˜ì—­",
  "action_items": ["ì¡°ì¹˜1", "ì¡°ì¹˜2", "ì¡°ì¹˜3"],
  "timeline": "ì˜ˆìƒ ì˜í–¥ ê¸°ê°„ (ì˜ˆ: 1-2ì£¼)"
}"""

        try:
            result = self._call_gpt(system_prompt, json.dumps(simulation_result, ensure_ascii=False))
            return json.loads(result)
        except Exception as e:
            logger.error(f"ì‹œë®¬ë ˆì´ì…˜ í•´ì„ ì‹¤íŒ¨: {e}")
            return self._fallback_interpret_simulation(simulation_result)

    def _fallback_interpret_simulation(self, result: Dict) -> Dict[str, Any]:
        """ì‹œë®¬ë ˆì´ì…˜ í•´ì„ í´ë°±"""
        return {
            "impact_summary": "ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ì— ëŒ€í•œ ìƒì„¸ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.",
            "most_affected": "ë°ì´í„° ë¶„ì„ í•„ìš”",
            "action_items": ["ê²°ê³¼ ê²€í† ", "ë‹´ë‹¹ì ë…¼ì˜", "ëŒ€ì‘ ë°©ì•ˆ ìˆ˜ë¦½"],
            "timeline": "ë¶„ì„ í•„ìš”"
        }

    # ========================================
    # 5. ì „ì´ ê²½ë¡œ ì„¤ëª…
    # ========================================
    def explain_propagation(self, propagation_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë¦¬ìŠ¤í¬ ì „ì´ ê²½ë¡œ â†’ ë¹„ì¦ˆë‹ˆìŠ¤ ì„¤ëª…"""

        if not self.is_available:
            return self._fallback_explain_propagation(propagation_data)

        system_prompt = """ë‹¹ì‹ ì€ ê³µê¸‰ë§ ë¦¬ìŠ¤í¬ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
ë¦¬ìŠ¤í¬ ì „ì´ ê²½ë¡œë¥¼ ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ì—ì„œ ì„¤ëª…í•˜ì„¸ìš”.

ì •í™•íˆ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{
  "pathway_explanation": "ì „ì´ ê²½ë¡œ ì„¤ëª… (100ì ì´ë‚´)",
  "critical_nodes": ["í•µì‹¬ ë…¸ë“œ1", "í•µì‹¬ ë…¸ë“œ2"],
  "risk_level": "high|medium|low",
  "mitigation": "ì™„í™” ë°©ì•ˆ (50ì ì´ë‚´)"
}"""

        try:
            result = self._call_gpt(system_prompt, json.dumps(propagation_data, ensure_ascii=False))
            return json.loads(result)
        except Exception as e:
            logger.error(f"ì „ì´ ê²½ë¡œ ì„¤ëª… ì‹¤íŒ¨: {e}")
            return self._fallback_explain_propagation(propagation_data)

    def _fallback_explain_propagation(self, data: Dict) -> Dict[str, Any]:
        """ì „ì´ ê²½ë¡œ ì„¤ëª… í´ë°±"""
        propagators = data.get("topPropagators", [])
        nodes = [p.get("company", "") for p in propagators[:2]]

        return {
            "pathway_explanation": "ê³µê¸‰ë§ì„ í†µí•´ ë¦¬ìŠ¤í¬ê°€ ì „ì´ë˜ê³  ìˆìŠµë‹ˆë‹¤. ìƒì„¸ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.",
            "critical_nodes": nodes if nodes else ["ë¶„ì„ í•„ìš”"],
            "risk_level": "medium",
            "mitigation": "ê³µê¸‰ë§ ë‹¤ë³€í™” ë° ëª¨ë‹ˆí„°ë§ ê°•í™” í•„ìš”"
        }

    # ========================================
    # 6. ëŒ€ì‘ ì „ëµ (RM/OPS ê°€ì´ë“œ)
    # ========================================
    def generate_action_guide(self, deal_data: Dict[str, Any], signal_type: str = "OPERATIONAL") -> Dict[str, Any]:
        """ìƒí™©ë³„ RM/OPS ê°€ì´ë“œ ìƒì„±"""

        if not self.is_available:
            return self._fallback_action_guide(deal_data, signal_type)

        system_prompt = f"""ë‹¹ì‹ ì€ PE/IB ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
{signal_type} ìƒí™©ì—ì„œì˜ ëŒ€ì‘ ê°€ì´ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

ì •í™•íˆ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "rm_guide": {{
    "summary": "RM ëŒ€ì‘ ìš”ì•½ (30ì ì´ë‚´)",
    "todo_list": ["í• ì¼1", "í• ì¼2", "í• ì¼3"],
    "talking_points": ["ê³ ê° ëŒ€í™” í¬ì¸íŠ¸1", "í¬ì¸íŠ¸2"]
  }},
  "ops_guide": {{
    "summary": "OPS ëŒ€ì‘ ìš”ì•½ (30ì ì´ë‚´)",
    "todo_list": ["í• ì¼1", "í• ì¼2", "í• ì¼3"],
    "financial_impact": "ì¬ë¬´ ì˜í–¥ ë¶„ì„ (í•œ ë¬¸ì¥)"
  }},
  "urgency": "immediate|within_24h|within_week"
}}"""

        try:
            result = self._call_gpt(system_prompt, json.dumps(deal_data, ensure_ascii=False))
            return json.loads(result)
        except Exception as e:
            logger.error(f"ëŒ€ì‘ ê°€ì´ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._fallback_action_guide(deal_data, signal_type)

    def _fallback_action_guide(self, deal_data: Dict, signal_type: str) -> Dict[str, Any]:
        """ëŒ€ì‘ ê°€ì´ë“œ í´ë°±"""
        guides = {
            "LEGAL_CRISIS": {
                "rm_guide": {
                    "summary": "ë²•ë¥  ë¦¬ìŠ¤í¬ ëŒ€ì‘ í•„ìš”",
                    "todo_list": ["ê³ ê° ë©´ë‹´", "ë¦¬ìŠ¤í¬ ê³µìœ ", "ë²•ë¬´ ê²€í† "],
                    "talking_points": ["í˜„ì¬ ìƒí™© ì„¤ëª…", "ëŒ€ì‘ ë°©ì•ˆ ê³µìœ "]
                },
                "ops_guide": {
                    "summary": "ì¶©ë‹¹ê¸ˆ ë° ê³„ì•½ ê²€í† ",
                    "todo_list": ["EOD ì¤€ë¹„", "ìì‚° ì ê²€", "ë²•ë¬´ë²•ì¸ í˜‘ì˜"],
                    "financial_impact": "ì†í•´ë°°ìƒ ì‹œë‚˜ë¦¬ì˜¤ë³„ ì˜í–¥ ë¶„ì„ í•„ìš”"
                },
                "urgency": "immediate"
            },
            "MARKET_CRISIS": {
                "rm_guide": {
                    "summary": "ì‹œì¥ ë¦¬ìŠ¤í¬ ëª¨ë‹ˆí„°ë§ ê°•í™”",
                    "todo_list": ["ì‹œì¥ ë™í–¥ ë¶„ì„", "ìœ ë™ì„± í™•ë³´", "í—¤ì§€ ê²€í† "],
                    "talking_points": ["ì‹œì¥ ì „ë§ ê³µìœ ", "ëŒ€ì‘ ì „ëµ ë…¼ì˜"]
                },
                "ops_guide": {
                    "summary": "LTV ë° ë‹´ë³´ ì¬ê²€í† ",
                    "todo_list": ["ë‹´ë³´ ê°€ì¹˜ ì¬í‰ê°€", "Covenant ì ê²€", "ë¹„ìƒ ì‹œë‚˜ë¦¬ì˜¤"],
                    "financial_impact": "ì‹œì¥ ë³€ë™ì— ë”°ë¥¸ í¬íŠ¸í´ë¦¬ì˜¤ ì˜í–¥ ë¶„ì„ í•„ìš”"
                },
                "urgency": "within_24h"
            }
        }

        return guides.get(signal_type, {
            "rm_guide": {
                "summary": "ì •ê¸° ëª¨ë‹ˆí„°ë§ ìˆ˜í–‰",
                "todo_list": ["í˜„í™© ì ê²€", "ë³´ê³ ì„œ ê²€í† ", "ì´ìŠˆ ì¶”ì "],
                "talking_points": ["ì •ê¸° ì—…ë°ì´íŠ¸ ê³µìœ "]
            },
            "ops_guide": {
                "summary": "ìš´ì˜ í˜„í™© ì ê²€",
                "todo_list": ["KPI ì ê²€", "ë¦¬ìŠ¤í¬ ì§€í‘œ í™•ì¸", "ë³´ê³ ì„œ ì‘ì„±"],
                "financial_impact": "íŠ¹ì´ì‚¬í•­ ì—†ìŒ"
            },
            "urgency": "within_week"
        })


    # ========================================
    # 7. ì¢…í•© ì¸ì‚¬ì´íŠ¸ ë¶„ì„ (NEW)
    # ========================================
    def generate_comprehensive_insight(self, deal_context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë”œ ëŒ€ìƒì— ëŒ€í•œ ì¢…í•© ì¸ì‚¬ì´íŠ¸ ìƒì„±

        ë¦¬ìŠ¤í¬ ì ìˆ˜ëŠ” ì´ë¯¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê³„ì‚°ë˜ì–´ ìˆìœ¼ë¯€ë¡œ,
        AIëŠ” ë§¥ë½ì  í•´ì„, íŒ¨í„´ ì¸ì‹, êµì°¨ ë¶„ì„, ê¶Œê³ ì‚¬í•­ì„ ì œê³µ

        Args:
            deal_context: {
                "company": str,              # ê¸°ì—…ëª…
                "sector": str,               # ì„¹í„°
                "riskScore": int,            # í˜„ì¬ ë¦¬ìŠ¤í¬ ì ìˆ˜ (ì•Œê³ ë¦¬ì¦˜ ê³„ì‚°)
                "riskLevel": str,            # PASS/WARNING/FAIL
                "signals": [                 # ìµœê·¼ ìˆ˜ì§‘ëœ ì‹ í˜¸ë“¤
                    {
                        "type": "news|disclosure",
                        "category": str,
                        "title": str,
                        "score": int,
                        "date": str
                    }
                ],
                "executives": [              # ì„ì› ëª©ë¡
                    {"name": str, "position": str}
                ],
                "shareholders": [            # ì£¼ì£¼ ëª©ë¡
                    {"name": str, "shareRatio": float}
                ],
                "relatedCompanies": [        # ê´€ê³„ê¸°ì—…
                    {"name": str, "relation": str}
                ],
                "categoryScores": {          # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜
                    "financial": int,
                    "legal": int,
                    "governance": int,
                    "operational": int,
                    "market": int
                }
            }

        Returns:
            ì¢…í•© ì¸ì‚¬ì´íŠ¸ ê²°ê³¼
        """

        if not self.is_available:
            return self._fallback_comprehensive_insight(deal_context)

        system_prompt = """ë‹¹ì‹ ì€ PE/IB ë”œ ì‹¬ì‚¬ ì „ë¬¸ê°€ì´ì ë¦¬ìŠ¤í¬ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ë”œ ëŒ€ìƒì˜ ë°ì´í„°ë¥¼ ì¢…í•© ë¶„ì„í•˜ì—¬ **ì¸ì‚¬ì´íŠ¸**ë¥¼ ì œê³µí•˜ì„¸ìš”.
âš ï¸ ë¦¬ìŠ¤í¬ ì ìˆ˜ëŠ” ì´ë¯¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê³„ì‚°ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì ìˆ˜ ì¬ê³„ì‚°ì´ ì•„ë‹Œ **ë§¥ë½ì  í•´ì„**ì— ì§‘ì¤‘í•˜ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{
  "executive_summary": "í˜„ì¬ ìƒí™© í•µì‹¬ ìš”ì•½ (2-3ë¬¸ì¥, ì ìˆ˜ê°€ ì•„ë‹Œ ì˜ë¯¸ ì¤‘ì‹¬)",

  "context_analysis": {
    "industry_context": "í•´ë‹¹ ê¸°ì—…ì˜ ì—…ê³„ ìƒí™© ë§¥ë½ (ì‹œì¥ íŠ¸ë Œë“œ, ì—…í™© ë“±)",
    "timing_significance": "í˜„ ì‹œì ì˜ ì‹ í˜¸ë“¤ì´ ê°–ëŠ” ì‹œê¸°ì  ì˜ë¯¸"
  },

  "cross_signal_analysis": {
    "patterns_detected": ["ê°ì§€ëœ íŒ¨í„´1", "íŒ¨í„´2"],
    "correlations": "ì„œë¡œ ë‹¤ë¥¸ ì‹ í˜¸ë“¤ ê°„ì˜ ì—°ê´€ì„± ë¶„ì„",
    "anomalies": "ì´ìƒ ì§•í›„ë‚˜ ì£¼ëª©í•  ì "
  },

  "stakeholder_insights": {
    "executive_concerns": "ì„ì›/ì§€ë°°êµ¬ì¡° ê´€ë ¨ ì‹œì‚¬ì  (í•´ë‹¹ì‹œ)",
    "shareholder_dynamics": "ì£¼ì£¼ êµ¬ì„±/ë³€ë™ ê´€ë ¨ ì‹œì‚¬ì  (í•´ë‹¹ì‹œ)"
  },

  "key_concerns": [
    {
      "issue": "ìš°ë ¤ ì‚¬í•­",
      "why_it_matters": "ì™œ ì¤‘ìš”í•œì§€",
      "watch_for": "ì•ìœ¼ë¡œ ì£¼ì‹œí•´ì•¼ í•  ì "
    }
  ],

  "recommendations": {
    "immediate_actions": ["ì¦‰ì‹œ í•„ìš”í•œ ì¡°ì¹˜1", "ì¡°ì¹˜2"],
    "monitoring_focus": ["ì§‘ì¤‘ ëª¨ë‹ˆí„°ë§ í•­ëª©1", "í•­ëª©2"],
    "due_diligence_points": ["ì¶”ê°€ ì‹¤ì‚¬ í•„ìš” í•­ëª©1", "í•­ëª©2"]
  },

  "confidence": 0.0-1.0,
  "analysis_limitations": "ë¶„ì„ì˜ í•œê³„ì ì´ë‚˜ ì¶”ê°€ë¡œ í•„ìš”í•œ ì •ë³´"
}"""

        user_prompt = f"""ë”œ ëŒ€ìƒ ë¶„ì„ ìš”ì²­:

ê¸°ì—…ëª…: {deal_context.get('company', 'N/A')}
ì„¹í„°: {deal_context.get('sector', 'N/A')}
í˜„ì¬ ë¦¬ìŠ¤í¬ ì ìˆ˜: {deal_context.get('riskScore', 'N/A')} (ì•Œê³ ë¦¬ì¦˜ ì‚°ì¶œ)
ë¦¬ìŠ¤í¬ ë“±ê¸‰: {deal_context.get('riskLevel', 'N/A')}

ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ (ì•Œê³ ë¦¬ì¦˜ ì‚°ì¶œ):
{json.dumps(deal_context.get('categoryScores', {}), ensure_ascii=False, indent=2)}

ìµœê·¼ ì‹ í˜¸ë“¤:
{json.dumps(deal_context.get('signals', [])[:10], ensure_ascii=False, indent=2)}

ì„ì› í˜„í™©:
{json.dumps(deal_context.get('executives', [])[:5], ensure_ascii=False, indent=2)}

ì£¼ìš” ì£¼ì£¼:
{json.dumps(deal_context.get('shareholders', [])[:5], ensure_ascii=False, indent=2)}

ê´€ê³„ ê¸°ì—…:
{json.dumps(deal_context.get('relatedCompanies', [])[:5], ensure_ascii=False, indent=2)}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
ì ìˆ˜ ê³„ì‚°ì´ ì•„ë‹Œ, ë§¥ë½ì  í•´ì„ê³¼ ì‹¤ë¬´ì  ì‹œì‚¬ì ì— ì§‘ì¤‘í•´ì£¼ì„¸ìš”."""

        try:
            result = self._call_gpt(system_prompt, user_prompt)
            parsed = json.loads(result)
            return parsed
        except Exception as e:
            logger.error(f"ì¢…í•© ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._fallback_comprehensive_insight(deal_context)

    def _fallback_comprehensive_insight(self, deal_context: Dict) -> Dict[str, Any]:
        """ì¢…í•© ì¸ì‚¬ì´íŠ¸ í´ë°± (AI ë¯¸ì‚¬ìš© ì‹œ)"""
        company = deal_context.get('company', 'ëŒ€ìƒ ê¸°ì—…')
        score = deal_context.get('riskScore', 50)
        level = deal_context.get('riskLevel', 'WARNING')
        signals = deal_context.get('signals', [])
        category_scores = deal_context.get('categoryScores', {})

        # ê°€ì¥ ë†’ì€ ì¹´í…Œê³ ë¦¬ ì°¾ê¸°
        max_category = max(category_scores, key=category_scores.get) if category_scores else "operational"
        max_score = category_scores.get(max_category, 0)

        # ìµœê·¼ ì‹ í˜¸ íŒ¨í„´ ë¶„ì„ (ê°„ë‹¨í•œ ë£° ê¸°ë°˜)
        recent_categories = [s.get('category', '') for s in signals[:10]]
        category_counts = {}
        for cat in recent_categories:
            category_counts[cat] = category_counts.get(cat, 0) + 1

        patterns = []
        if category_counts.get('legal', 0) >= 2:
            patterns.append("ë²•ì  ì´ìŠˆ ì‹ í˜¸ ì§‘ì¤‘ ë°œìƒ")
        if category_counts.get('governance', 0) >= 2:
            patterns.append("ì§€ë°°êµ¬ì¡° ê´€ë ¨ ì‹ í˜¸ ë‹¤ìˆ˜")
        if category_counts.get('financial', 0) >= 2:
            patterns.append("ì¬ë¬´ ê´€ë ¨ ì‹ í˜¸ ëˆ„ì ")

        # ìƒíƒœë³„ ê¶Œê³ ì‚¬í•­
        if level == 'FAIL':
            immediate_actions = ["ê¸´ê¸‰ ë‚´ë¶€ ê²€í†  íšŒì˜ ì†Œì§‘", "íˆ¬ììœ„ì›íšŒ ë³´ê³ "]
            monitoring_focus = ["ì¼ê°„ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§", "ê³µì‹œ ì‹¤ì‹œê°„ ì¶”ì "]
        elif level == 'WARNING':
            immediate_actions = ["ì£¼ê°„ ëª¨ë‹ˆí„°ë§ ë¹ˆë„ ìƒí–¥", "ë‹´ë‹¹ì ë©´ë‹´ ê²€í† "]
            monitoring_focus = ["í•µì‹¬ ì§€í‘œ ì¶”ì ", "ì—…ê³„ ë™í–¥ íŒŒì•…"]
        else:
            immediate_actions = ["ì •ê¸° ëª¨ë‹ˆí„°ë§ ìœ ì§€"]
            monitoring_focus = ["ì›”ê°„ ë¦¬í¬íŠ¸ ê²€í† "]

        return {
            "executive_summary": f"{company}ì€(ëŠ”) í˜„ì¬ {level} ë“±ê¸‰ìœ¼ë¡œ, {max_category} ì˜ì—­ì—ì„œ ê°€ì¥ ë†’ì€ ë¦¬ìŠ¤í¬ ì‹ í˜¸({max_score}ì )ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. AI ë¶„ì„ ë¶ˆê°€ë¡œ ìƒì„¸ ë§¥ë½ í•´ì„ì€ ì œí•œë©ë‹ˆë‹¤.",

            "context_analysis": {
                "industry_context": "AI ì„œë¹„ìŠ¤ ë¯¸ì—°ê²°ë¡œ ì—…ê³„ ë§¥ë½ ë¶„ì„ ë¶ˆê°€. ìˆ˜ë™ ê²€í†  í•„ìš”.",
                "timing_significance": f"ìµœê·¼ {len(signals)}ê±´ì˜ ì‹ í˜¸ê°€ ìˆ˜ì§‘ë¨. ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ í•„ìš”."
            },

            "cross_signal_analysis": {
                "patterns_detected": patterns if patterns else ["ëšœë ·í•œ íŒ¨í„´ ë¯¸ê°ì§€"],
                "correlations": "AI ë¶„ì„ í•„ìš”",
                "anomalies": "ìˆ˜ë™ ê²€í†  í•„ìš”"
            },

            "stakeholder_insights": {
                "executive_concerns": "ì„ì› ë°ì´í„° êµì°¨ ë¶„ì„ í•„ìš”" if deal_context.get('executives') else "ì„ì› ë°ì´í„° ì—†ìŒ",
                "shareholder_dynamics": "ì£¼ì£¼ ë³€ë™ ì¶”ì  í•„ìš”" if deal_context.get('shareholders') else "ì£¼ì£¼ ë°ì´í„° ì—†ìŒ"
            },

            "key_concerns": [
                {
                    "issue": f"{max_category} ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤í¬ ì§‘ì¤‘",
                    "why_it_matters": f"í•´ë‹¹ ì˜ì—­ ì ìˆ˜ê°€ {max_score}ì ìœ¼ë¡œ ì „ì²´ ë¦¬ìŠ¤í¬ì˜ ì£¼ìš” ì›ì¸",
                    "watch_for": "ê´€ë ¨ ì¹´í…Œê³ ë¦¬ ì‹ ê·œ ì‹ í˜¸ ë°œìƒ ì—¬ë¶€"
                }
            ],

            "recommendations": {
                "immediate_actions": immediate_actions,
                "monitoring_focus": monitoring_focus,
                "due_diligence_points": ["AI ì„œë¹„ìŠ¤ ì—°ê²° í›„ ìƒì„¸ ë¶„ì„ ê¶Œì¥"]
            },

            "confidence": 0.3,
            "analysis_limitations": "OpenAI API ë¯¸ì—°ê²°ë¡œ ë£° ê¸°ë°˜ ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰ë¨. ë§¥ë½ì  í•´ì„ ë° êµì°¨ ë¶„ì„ì„ ìœ„í•´ AI ì„œë¹„ìŠ¤ ì—°ê²° í•„ìš”."
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
ai_service_v2 = AIServiceV2()


# ========================================
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# ========================================
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    print(f"ğŸ¤– AI Service v2 Status: {'âœ… í™œì„±í™”' if ai_service_v2.is_available else 'âŒ ë¹„í™œì„±í™”'}")
    print(f"ğŸ“Œ Model: {ai_service_v2.model}")

    if ai_service_v2.is_available:
        # í…ŒìŠ¤íŠ¸: ë‰´ìŠ¤ ë¶„ì„
        print("\nğŸ§ª ë‰´ìŠ¤ ë¶„ì„ í…ŒìŠ¤íŠ¸:")
        result = ai_service_v2.analyze_news(
            "SKí•˜ì´ë‹‰ìŠ¤ê°€ ë¯¸êµ­ íŠ¹í—ˆ ì¹¨í•´ ì†Œì†¡ì— íœ˜ë§ë ¸ë‹¤.",
            "SKí•˜ì´ë‹‰ìŠ¤, ç¾ íŠ¹í—ˆ ì¹¨í•´ ì†Œì†¡ í”¼ì†Œ"
        )
        print(json.dumps(result, indent=2, ensure_ascii=False))

        # í…ŒìŠ¤íŠ¸: Text2Cypher
        print("\nğŸ§ª Text2Cypher í…ŒìŠ¤íŠ¸:")
        result = ai_service_v2.text_to_cypher("ê³ ìœ„í—˜ ê¸°ì—… ëª©ë¡ ë³´ì—¬ì¤˜")
        print(json.dumps(result, indent=2, ensure_ascii=False))
    else:
        print("\nâš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("í´ë°± í…ŒìŠ¤íŠ¸:")
        result = ai_service_v2._fallback_text_to_cypher("ê³ ìœ„í—˜ ê¸°ì—…")
        print(json.dumps(result, indent=2, ensure_ascii=False))
